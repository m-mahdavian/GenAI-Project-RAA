{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11301610,"sourceType":"datasetVersion","datasetId":7067777}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mmahdavian/capstone-raa-project?scriptVersionId=233714952\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Reviewer Assistant Agent (RAA) Notebook\n\n**This notebook builds an agent to assist reviewers by summarizing manuscriptsand comparing their key findings (corrosion inhibition, impedance, adhesion)with similar open-access articles found via internet search.**\n\n**This notebook implements a Reviewer Assistant Agent (RAA) using the LangChain framework and OpenAI's GPT-4-turbo model. The agent takes a manuscript in PDF format, extracts its text content, summarizes it focusing on specific quantitative results (corrosion inhibition %, impedance ohm.cm2, adhesion MPa), finds similar articles online, extracts the same metrics from those articles, and presents a comparative summary.**\n\n**Goal: To automate parts of the peer-review process by (a) Providing a concise summary of a manuscript's key quantitative findings related to corrosion inhibition, impedance, and adhesion.b) Identifying and extracting comparable results from the top relevant web search results for similar articles, facilitating a quick comparison of the manuscript's contribution against existing literature.**\n\n**Input: PDF manuscript file path, OpenAI API Key**\n\n**Framework: LangChain, OpenAI**\n\n**LLM: gpt-4-turbo, OpenAI**\n\n**Tools: File handling and text extraction, web searching and downloading (googlesearch-python, requests),**\n\n**Outputs: (1) Text summary of the input manuscript highlighting key metrics, (2) Pandas DataFrame containing links and extracted metrics from similar articles.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Installation ---\n# Install necessary libraries (run this cell first in Kaggle)\n!pip install -q langchain langchain_openai openai pypdf pandas google-search-results beautifulsoup4 kaggle googlesearch-python langchain_community requests lxml\nprint(\"Libraries installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:04:55.80208Z","iopub.execute_input":"2025-04-13T22:04:55.802497Z","iopub.status.idle":"2025-04-13T22:05:00.017005Z","shell.execute_reply.started":"2025-04-13T22:04:55.80247Z","shell.execute_reply":"2025-04-13T22:05:00.015315Z"}},"outputs":[{"name":"stdout","text":"Libraries installed.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# --- 2. Setup and Imports ---\nimport os\nimport re\nimport requests\nimport time\nimport pandas as pd\nfrom tempfile import NamedTemporaryFile\nfrom urllib.parse import urljoin, urlparse\n\nfrom bs4 import BeautifulSoup\nfrom kaggle_secrets import UserSecretsClient\n\n# LangChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.prompts import PromptTemplate\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Google Search\ntry:\n    from googlesearch import search\nexcept ImportError:\n    print(\"Could not import googlesearch. Please ensure it's installed.\")\n    search = None\n\nprint(\"Imports successful.\")\n\n# --- 3. Configuration ---\ntry:\n    user_secrets = UserSecretsClient()\n    OPENAI_API_KEY = user_secrets.get_secret(\"OpenAI_API_KEY\")\n    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n    print(\"OpenAI API Key retrieved successfully.\")\nexcept Exception as e:\n    print(f\"Error retrieving OpenAI API Key from Kaggle Secrets: {e}\")\n    OPENAI_API_KEY = None\n\n# Constants\nPDF_MANUSCRIPT_PATH = \"/kaggle/input/sample-manuscript/Sample-Manuscript.pdf\" # <-- Verify\nLLM_MODEL = \"gpt-4-turbo\"\nMAX_SEARCH_RESULTS = 20\nDOWNLOAD_TIMEOUT = 25 # Slightly longer timeout\nDOWNLOAD_DIR = \"/kaggle/working/temp_pdfs\"\nHEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n# Max characters to send to LLM for extraction task (balance context vs token limits)\nMAX_CHARS_FOR_EXTRACTION = 25000\n\n# --- 4. Initialize LLM ---\nif OPENAI_API_KEY:\n    llm = ChatOpenAI(temperature=0, model_name=LLM_MODEL, openai_api_key=OPENAI_API_KEY, request_timeout=120) # Increased LLM timeout\n    print(f\"LLM ({LLM_MODEL}) initialized.\")\nelse:\n    llm = None\n    print(\"LLM could not be initialized.\")\n\n# --- 5. Helper Functions ---\n\ndef load_pdf_text(pdf_path):\n    \"\"\"Loads text content from a PDF file. Returns (full_text, docs_list)\"\"\"\n    try:\n        print(f\"Loading PDF: {pdf_path}\")\n        loader = PyPDFLoader(pdf_path, extract_images=False) # Ignore images\n        docs = loader.load()\n        if not docs:\n             print(f\"Warning: PyPDFLoader loaded 0 pages from {pdf_path}.\")\n             return None, None\n        # Clean potentially problematic characters before joining\n        cleaned_pages = []\n        for doc in docs:\n             # Replace null bytes and normalize whitespace\n             cleaned_content = doc.page_content.replace('\\x00', '').strip()\n             cleaned_pages.append(cleaned_content)\n\n        full_text = \"\\n\".join(filter(None, cleaned_pages)) # Join non-empty pages\n\n        if not full_text:\n            print(f\"Warning: PDF loaded but resulted in empty text content after cleaning: {pdf_path}\")\n            return None, docs # Return docs even if text is empty\n\n        print(f\"Successfully loaded and cleaned text from: {pdf_path} ({len(docs)} pages, {len(full_text)} chars)\")\n        return full_text, docs\n    except Exception as e:\n        print(f\"Error loading PDF {pdf_path}: {type(e).__name__} - {e}\")\n        return None, None\n\ndef extract_text_from_html(html_content):\n    \"\"\"Extracts plain text from HTML content.\"\"\"\n    try:\n        soup = BeautifulSoup(html_content, 'lxml')\n        # Remove script and style elements\n        for script_or_style in soup([\"script\", \"style\"]):\n            script_or_style.decompose()\n        # Get text, separate paragraphs, and clean up whitespace\n        text = soup.get_text(separator='\\n', strip=True)\n        # Optional: Further cleaning (e.g., removing excessive blank lines)\n        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n        print(f\"Extracted ~{len(text)} characters of text from HTML.\")\n        return text\n    except Exception as e:\n        print(f\"Error extracting text from HTML: {e}\")\n        return None\n\ndef extract_manuscript_title(pdf_docs):\n    \"\"\"Extracts manuscript title using LLM or filename fallback.\"\"\"\n    fallback_title = \"Unknown Manuscript Title\"\n    if PDF_MANUSCRIPT_PATH and os.path.exists(PDF_MANUSCRIPT_PATH):\n        base_name = os.path.basename(PDF_MANUSCRIPT_PATH)\n        title, _ = os.path.splitext(base_name)\n        fallback_title = title.replace('-', ' ').replace('_', ' ') # Use filename as primary fallback\n        print(f\"Prepared fallback title from filename: {fallback_title}\")\n\n    if not llm or not pdf_docs:\n        print(\"LLM not available or no document content for title extraction. Using fallback.\")\n        return fallback_title\n\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n    split_docs = text_splitter.split_documents(pdf_docs[:5]) # Use first few pages\n\n    prompt_template = \"\"\"\n    Based on the following text from the beginning of a scientific manuscript, what is its exact full title? Only return the title itself, nothing else.\n\n    Text:\n    \"{text}\"\n\n    Title:\"\"\"\n    prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n\n    if split_docs:\n        try:\n            context_text = \"\\n\".join([doc.page_content for doc in split_docs])[:4000]\n            title_query = prompt.format(text=context_text)\n            response = llm.invoke(title_query)\n            title = response.content.strip().strip('\"')\n            if 5 < len(title) < 300:\n                print(f\"Extracted title via LLM: {title}\")\n                return title\n            else:\n                 print(f\"LLM extraction yielded unusual title. Using fallback: {fallback_title}\")\n                 return fallback_title\n        except Exception as e:\n            print(f\"Error during LLM title extraction: {e}. Using fallback: {fallback_title}\")\n            return fallback_title\n    else:\n         print(\"No text chunks available for LLM title extraction. Using fallback.\")\n         return fallback_title\n\n\ndef summarize_and_extract_manuscript_data(docs, full_text):\n    \"\"\"Summarizes the manuscript and extracts key data using LLM.\"\"\"\n    if not llm: return \"LLM not available.\", {}\n    if not docs and not full_text: return \"No content provided.\", {}\n\n    summary = \"Summary generation failed.\"\n    extracted_values = {}\n\n    # --- Summarization ---\n    if docs:\n        try:\n            print(\"Generating summary...\")\n            summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n            text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=500)\n            split_docs = text_splitter.split_documents(docs)\n            if split_docs:\n                summary_result = summary_chain.invoke(split_docs)\n                summary = summary_result.get(\"output_text\", summary)\n                print(\"Summary generated.\")\n            else:\n                 summary = \"Document splitting failed for summary.\"\n        except Exception as e:\n            print(f\"Error during summarization: {e}\")\n            summary = f\"Error during summarization: {e}\"\n    else:\n        summary = \"Original document list not available for summarization.\"\n\n\n    # --- Extraction ---\n    if full_text:\n        try:\n            print(\"Extracting specific data points from manuscript text...\")\n            # Use the dedicated extraction function\n            extracted_values = extract_data_from_text(full_text, source_type=\"Manuscript PDF\")\n        except Exception as e:\n            print(f\"Error during manuscript data extraction: {e}\")\n            # Ensure default values if extraction fails completely\n            extracted_values = {\n                'corrosion_inhibition_%': 'Extraction Error',\n                'impedance_ohm_cm2': 'Extraction Error',\n                'adhesion_MPa': 'Extraction Error'\n                }\n    else:\n        print(\"No full text available for manuscript data extraction.\")\n        extracted_values = {key: 'No Text' for key in ['corrosion_inhibition_%', 'impedance_ohm_cm2', 'adhesion_MPa']}\n\n\n    return summary, extracted_values\n\n\ndef extract_data_from_text(text_content, source_type=\"Unknown Source\"):\n    \"\"\"Extracts key data points from provided text using LLM.\"\"\"\n    if not llm:\n        print(f\"LLM not available for data extraction from {source_type}.\")\n        return {}\n    if not text_content:\n        print(f\"No text content provided for extraction from {source_type}.\")\n        return {}\n\n    # --- Updated Extraction Prompt ---\n    extraction_prompt_template = \"\"\"\n    Analyze the following text from a scientific paper ({source_type}). Extract the following specific quantitative data points if present. Look carefully throughout the text, especially in results, discussion, and conclusion sections. If a value is explicitly mentioned, report it. If not found, state 'Not Found'. Provide only the numerical value (e.g., 95.5, 1.2E6, 15.3) or 'Not Found'.\n\n    1.  **Corrosion Inhibition Efficiency (%):** Look for 'Corrosion Inhibition Efficiency', 'Inhibition Efficiency', or 'IE%'. Report the highest or most representative value. [Value or 'Not Found']\n    2.  **Impedance Magnitude (ohm.cm2 or ꭥ.cm2):** Look for impedance values, often associated with EIS results (e.g., Z_real, |Z|, R_ct, R_p) reported in Ohm.cm² (or kohm.cm², Mohm.cm² - convert if possible, otherwise state units). Report a key value (e.g., low frequency, after exposure). [Value (specify units if not ohm.cm2) or 'Not Found']\n    3.  **Adhesion Strength (MPa):** Look for results from pull-off, scratch, or similar adhesion tests reported in MPa. [Value or 'Not Found']\n\n    Format the output clearly, one item per line:\n    Corrosion Inhibition (%): [Value or 'Not Found']\n    Impedance (ohm.cm2): [Value or 'Not Found']\n    Adhesion (MPa): [Value or 'Not Found']\n\n    Text Snippet:\n    \"{text}\"\n\n    Extracted Data:\n    \"\"\"\n    extraction_prompt = PromptTemplate(\n        template=extraction_prompt_template,\n        input_variables=[\"text\", \"source_type\"]\n    )\n\n    try:\n        print(f\"Requesting LLM extraction from {source_type}...\")\n        # Limit text sent to LLM to manage token usage/cost\n        text_snippet = text_content[:MAX_CHARS_FOR_EXTRACTION]\n\n        extraction_query = extraction_prompt.format(text=text_snippet, source_type=source_type)\n        extraction_response = llm.invoke(extraction_query)\n        extracted_data_text = extraction_response.content\n        print(f\"Extraction response received from LLM for {source_type}.\")\n\n        # Parse the LLM's response\n        extracted_values = parse_llm_extraction(extracted_data_text)\n        return extracted_values\n\n    except Exception as e:\n        print(f\"Error during LLM data extraction from {source_type}: {e}\")\n        # Return dict indicating error for this source\n        return {\n            'corrosion_inhibition_%': f'LLM Error ({source_type})',\n            'impedance_ohm_cm2': f'LLM Error ({source_type})',\n            'adhesion_MPa': f'LLM Error ({source_type})'\n        }\n\ndef search_similar_articles(query, num_results):\n    \"\"\"Performs Google search (no date filter).\"\"\"\n    if not search: return []\n    urls = []\n    print(f\"Searching Google for: '{query}' (Top {num_results})\")\n    try:\n        search_results = search(query, num_results=num_results, lang=\"en\") # No pause or tbs\n        urls = list(search_results)\n        print(f\"Found {len(urls)} potential URLs.\")\n    except Exception as e:\n        print(f\"Error during Google search: {e}\")\n        if \"429\" in str(e): print(\"Google search blocked (429).\")\n        urls = []\n    return urls\n\ndef download_pdf(url, save_dir):\n    \"\"\"Attempts direct PDF download. Returns (pdf_path, response_object_or_None)\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    pdf_path = None\n    response = None\n    try:\n        print(f\"Attempting direct PDF download from: {url}\")\n        response = requests.get(url, stream=True, timeout=DOWNLOAD_TIMEOUT, headers=HEADERS, allow_redirects=True)\n        response.raise_for_status()\n        content_type = response.headers.get('Content-Type', '').lower()\n        is_pdf_content = 'application/pdf' in content_type or 'pdf' in urlparse(url).path.lower()\n\n        if is_pdf_content and response.status_code == 200:\n            if int(response.headers.get('content-length', 1)) == 0:\n                 print(f\"Warning: Content-Length is 0 for {url}. Skipping download.\")\n                 return None, response\n\n            with NamedTemporaryFile(delete=False, suffix=\".pdf\", dir=save_dir) as temp_file:\n                bytes_downloaded = 0\n                for chunk in response.iter_content(chunk_size=8192):\n                    temp_file.write(chunk)\n                    bytes_downloaded += len(chunk)\n                pdf_path = temp_file.name\n\n            if bytes_downloaded < 1024:\n                 print(f\"Warning: Downloaded PDF {pdf_path} is small ({bytes_downloaded} bytes).\")\n\n            try:\n                 loader_check = PyPDFLoader(pdf_path, extract_images=False)\n                 loader_check.load() # Verify it's loadable\n                 print(f\"Successfully downloaded and verified PDF ({bytes_downloaded} bytes) to: {pdf_path}\")\n                 return pdf_path, response\n            except Exception as pdf_err:\n                 print(f\"Downloaded file {pdf_path} failed verification: {pdf_err}. Removing.\")\n                 if os.path.exists(pdf_path): os.remove(pdf_path)\n                 return None, response # Return None path, but keep response obj\n        else:\n            print(f\"URL does not appear to be a direct PDF. Status: {response.status_code}, Content-Type: {content_type}\")\n            return None, response\n\n    except requests.exceptions.Timeout:\n        print(f\"Timeout occurred downloading {url}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed download/access for {url}: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error during direct download {url}: {e}\")\n    # Return None path, but the response object if we got one\n    return None, response\n\n\ndef find_and_download_pdf_from_html(page_url, page_content, save_dir):\n    \"\"\"Parses HTML to find PDF links and attempts download.\"\"\"\n    print(f\"Scanning HTML from {page_url} for PDF links.\")\n    soup = BeautifulSoup(page_content, 'lxml')\n    pdf_links = []\n    found_links = set()\n\n    for a_tag in soup.find_all('a', href=True):\n        href = a_tag['href'].strip()\n        href_lower = href.lower()\n        link_text = a_tag.get_text(strip=True).lower()\n\n        is_potential_pdf_link = (\n            href_lower.endswith('.pdf') or\n            '/pdf' in href_lower or # More general path check\n            'download=pdf' in href_lower or\n            'downloadpdf' in href_lower or\n            'epdf' in href_lower or # Common for ePDFs\n            link_text in ['pdf', 'full text pdf', 'download pdf', '[pdf]', 'article pdf'] or\n            (a_tag.get('class') and any('pdf' in c.lower() for c in a_tag.get('class')))\n           )\n\n        if is_potential_pdf_link:\n            pdf_url = urljoin(page_url, href)\n            parsed_url = urlparse(pdf_url)\n            if parsed_url.scheme in ['http', 'https'] and not pdf_url.lower().startswith('javascript:'):\n                if pdf_url != page_url and pdf_url not in found_links:\n                    # Basic check to avoid linking to abstract/html pages named 'pdf'\n                    if '.pdf' in pdf_url or 'download' in pdf_url.lower() or 'content' in pdf_url.lower():\n                         print(f\"Found potential PDF link: {pdf_url}\")\n                         pdf_links.append(pdf_url)\n                         found_links.add(pdf_url)\n\n    # Prioritize links ending directly in .pdf\n    pdf_links.sort(key=lambda x: not x.lower().endswith('.pdf'))\n\n    if not pdf_links:\n        print(\"No likely PDF links found in HTML.\")\n        return None\n\n    print(f\"Found {len(pdf_links)} potential links. Attempting download...\")\n    for pdf_url in pdf_links:\n        time.sleep(0.5)\n        print(f\"Attempting download from HTML link: {pdf_url}\")\n        downloaded_path, _ = download_pdf(pdf_url, save_dir)\n        if downloaded_path:\n            print(f\"Successfully downloaded PDF from HTML link: {pdf_url}\")\n            return downloaded_path\n\n    print(\"Tried all potential PDF links from HTML, none worked.\")\n    return None\n\ndef parse_llm_extraction(llm_output_text):\n    \"\"\"Parses the LLM text output using regex for key data.\"\"\"\n    data = {\n        'corrosion_inhibition_%': 'Not Found',\n        'impedance_ohm_cm2': 'Not Found',\n        'adhesion_MPa': 'Not Found'\n    }\n    # Adjusted Regex to be slightly more flexible and capture units for impedance\n    patterns = {\n        'corrosion_inhibition_%': r\"Corrosion Inhibition \\(%\\):\\s*([\\d\\.]+%?|Not Found|N/A|-)\",\n        'impedance_ohm_cm2': r\"Impedance \\((?:ohm\\.cm2|ꭥ\\.cm2)\\):\\s*(.*?)(?:\\n|Adhesion|$)\", # Capture value potentially with units\n        'adhesion_MPa': r\"Adhesion \\(MPa\\):\\s*([\\d\\.]+|Not Found|N/A|-)\"\n    }\n\n    for key, pattern in patterns.items():\n        match = re.search(pattern, llm_output_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)\n        if match:\n            value = match.group(1).strip()\n            if value.lower() in ['not found', 'n/a', '-']:\n                data[key] = 'Not Found'\n            else:\n                # For impedance, keep the raw value as LLM might include units\n                if key == 'impedance_ohm_cm2':\n                     # Basic cleanup of impedance value\n                     value = re.sub(r'\\s+', ' ', value).strip('., ')\n                     if not value: # If value becomes empty after cleanup\n                          data[key] = 'Not Found'\n                     else:\n                          data[key] = value # Keep potentially complex value like \"1.5 Mohm.cm2\" or \"15000\"\n                else: # For % and MPa, extract number\n                    num_match = re.search(r\"([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)\", value)\n                    if num_match:\n                        data[key] = num_match.group(1).strip()\n                    else:\n                        data[key] = 'Not Found (parse failed)'\n                        print(f\"Info: Regex matched for {key} but couldn't parse number from '{value}'\")\n\n    print(f\"Parsed LLM Output: {data}\")\n    return data\n\n\n# --- 6. Main Execution ---\n\nif __name__ == \"__main__\":\n    if not llm:\n        print(\"Execution cannot proceed: LLM not initialized.\")\n    else:\n        # --- Part 1: Process the Manuscript ---\n        print(\"\\n--- Processing Manuscript ---\")\n        manuscript_full_text, manuscript_docs = load_pdf_text(PDF_MANUSCRIPT_PATH)\n        manuscript_summary = \"Summary N/A\"\n        manuscript_data = {}\n        manuscript_title = \"Title N/A\"\n\n        if manuscript_docs or manuscript_full_text:\n            # Extract title (needs docs ideally, but pass None if only text exists)\n            manuscript_title = extract_manuscript_title(manuscript_docs if manuscript_docs else None)\n            # Summarize and extract data (pass both docs and full_text)\n            manuscript_summary, manuscript_data = summarize_and_extract_manuscript_data(manuscript_docs, manuscript_full_text)\n        else:\n            print(f\"Critical Failure: Could not load text or docs from manuscript: {PDF_MANUSCRIPT_PATH}\")\n            # Attempt title extraction from filename as last resort\n            if PDF_MANUSCRIPT_PATH and os.path.exists(PDF_MANUSCRIPT_PATH):\n                 base_name = os.path.basename(PDF_MANUSCRIPT_PATH)\n                 title_f, _ = os.path.splitext(base_name)\n                 manuscript_title = title_f.replace('-', ' ').replace('_', ' ')\n                 print(f\"Using filename as title due to load failure: {manuscript_title}\")\n            else:\n                 manuscript_title = \"Unknown Manuscript Title\"\n\n\n        print(\"\\n--- Manuscript Summary ---\")\n        print(manuscript_summary)\n        print(\"\\n--- Manuscript Extracted Data ---\")\n        manuscript_df_data = {k: [v] for k, v in manuscript_data.items()} # Wrap values in lists for DataFrame\n        print(pd.DataFrame(manuscript_df_data))\n\n        # --- Part 2: Find and Process Similar Articles ---\n        print(f\"\\n--- Searching for Similar Articles (Title: '{manuscript_title}') ---\")\n        if manuscript_title.startswith(\"Unknown\") or manuscript_title == \"Title N/A\":\n             print(\"Skipping search due to missing/invalid manuscript title.\")\n             similar_article_urls = []\n        else:\n             similar_article_urls = search_similar_articles(manuscript_title, MAX_SEARCH_RESULTS)\n\n\n        similar_articles_data = []\n        processed_url_count = 0\n        successful_extractions = 0 # Count URLs where data extraction was attempted\n\n        os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n\n        if similar_article_urls:\n            for url in similar_article_urls:\n                processed_url_count += 1\n                print(f\"\\n--- Processing URL ({processed_url_count}/{len(similar_article_urls)}): {url} ---\")\n\n                pdf_path = None\n                response_obj = None\n                article_text = None\n                extracted_data = {}\n                status = \"Processing Failed\"\n                source_type = \"N/A\" # Where did we get the text for extraction?\n\n                try:\n                    # Attempt 1: Direct Download PDF\n                    pdf_path, response_obj = download_pdf(url, DOWNLOAD_DIR)\n\n                    if pdf_path:\n                        # Attempt 1a: Load text from downloaded PDF\n                        article_text, _ = load_pdf_text(pdf_path)\n                        if article_text:\n                            status = \"PDF Downloaded & Text Loaded\"\n                            source_type = \"PDF\"\n                        else:\n                             status = \"PDF Downloaded but Text Extraction Failed\"\n                             # Keep pdf_path for cleanup, but article_text is None\n                    else:\n                        # Attempt 2: Check if it was HTML\n                        if response_obj and 'text/html' in response_obj.headers.get('Content-Type', '').lower():\n                            html_content = response_obj.text\n                            if len(html_content) > 200: # Basic check for valid HTML page\n                                print(\"Direct download failed, content is HTML.\")\n                                status = \"HTML Page Found\"\n                                # Attempt 2a: Find and download PDF from HTML\n                                pdf_path_from_html = find_and_download_pdf_from_html(url, html_content, DOWNLOAD_DIR)\n                                if pdf_path_from_html:\n                                    pdf_path = pdf_path_from_html # Assign to main pdf_path for cleanup\n                                    article_text, _ = load_pdf_text(pdf_path)\n                                    if article_text:\n                                        status = \"PDF Link in HTML Found, Downloaded & Text Loaded\"\n                                        source_type = \"PDF (from HTML link)\"\n                                    else:\n                                        status = \"PDF via HTML Downloaded but Text Extraction Failed\"\n                                else:\n                                    # Attempt 2b: FALLBACK - Extract text directly from HTML\n                                    print(\"Could not download PDF from HTML links. Attempting extraction from HTML text.\")\n                                    status = \"HTML Scan Failed to Find PDF, Trying HTML Text Extraction\"\n                                    article_text = extract_text_from_html(html_content)\n                                    if article_text:\n                                        source_type = \"HTML\"\n                                    else:\n                                        status = \"HTML Scan Failed, HTML Text Extraction Failed\"\n                            else:\n                                status = \"HTML Page Found, but Content too small (Error Page?)\"\n                        elif response_obj:\n                             status = f\"Direct Download Failed (Status: {response_obj.status_code}, Type: {response_obj.headers.get('Content-Type', 'N/A')})\"\n                        else:\n                             status = \"Direct Download Failed (Network/Request Error)\"\n\n\n                    # If we managed to get text (from PDF or HTML), try extracting data\n                    if article_text:\n                        extracted_data = extract_data_from_text(article_text, source_type=source_type)\n                        successful_extractions += 1\n                        # Refine status based on extraction result\n                        if any(v not in ['Not Found', 'N/A', 'Extraction Error', 'No Text', 'LLM Error'] for v in extracted_data.values()):\n                             status += \" - Data Extraction Successful\"\n                        else:\n                             status += \" - Data Extraction Attempted (No Values Found by LLM)\"\n                    else:\n                         # If no text was extracted, create empty data structure\n                         extracted_data = {key: 'No Text Loaded' for key in ['corrosion_inhibition_%', 'impedance_ohm_cm2', 'adhesion_MPa']}\n\n\n                except Exception as process_err:\n                     print(f\"Unexpected error processing URL {url}: {process_err}\")\n                     status = \"Unexpected Error During Processing\"\n                     extracted_data = {key: 'Processing Error' for key in ['corrosion_inhibition_%', 'impedance_ohm_cm2', 'adhesion_MPa']}\n\n                finally:\n                    # Record the outcome\n                    result = {\n                        'url': url,\n                        'status': status,\n                        # Merge extracted data, ensuring keys exist\n                        **{key: extracted_data.get(key, 'N/A') for key in ['corrosion_inhibition_%', 'impedance_ohm_cm2', 'adhesion_MPa']}\n                    }\n                    similar_articles_data.append(result)\n\n                    # Clean up downloaded temporary file (PDF) if it exists\n                    if pdf_path and os.path.exists(pdf_path):\n                        try:\n                            os.remove(pdf_path)\n                            print(f\"Removed temporary file: {pdf_path}\")\n                        except OSError as e:\n                            print(f\"Error removing temporary file {pdf_path}: {e}\")\n\n                # Politeness delay\n                time.sleep(2.0) # Slightly increased delay\n\n        else:\n            print(\"No similar article URLs found or search failed/skipped.\")\n\n        # --- Part 3: Display Results ---\n        print(\"\\n\\n--- Final Results ---\")\n        print(\"\\n--- Manuscript Summary & Data ---\")\n        print(f\"Title: {manuscript_title}\")\n        print(\"Summary:\")\n        print(manuscript_summary)\n        print(\"\\nData:\")\n        print(pd.DataFrame({k: [v] for k, v in manuscript_data.items()}))\n\n        print(f\"\\n--- Similar Articles Data ({successful_extractions} URLs processed for extraction out of {processed_url_count} URLs checked) ---\")\n        if similar_articles_data:\n            df_similar_articles = pd.DataFrame(similar_articles_data)\n            column_order = ['url', 'status', 'corrosion_inhibition_%', 'impedance_ohm_cm2', 'adhesion_MPa']\n            df_similar_articles = df_similar_articles.reindex(columns=column_order, fill_value='N/A') # Ensure all columns exist & order\n\n            print(\"Results Table (Markdown):\")\n            print(df_similar_articles.to_markdown(index=False))\n            try:\n                from IPython.display import display\n                print(\"\\nDataFrame View:\")\n                display(df_similar_articles)\n            except ImportError: pass\n\n            # Optional: Save to CSV\n            # csv_output_path = \"/kaggle/working/similar_articles_data.csv\"\n            # df_similar_articles.to_csv(csv_output_path, index=False)\n            # print(f\"\\nSimilar articles data saved to {csv_output_path}\")\n        else:\n            print(\"No data collected from similar articles.\")\n\n        # Optional: Cleanup Temp Dir\n        # import shutil\n        # if os.path.exists(DOWNLOAD_DIR): shutil.rmtree(DOWNLOAD_DIR)\n\n\nprint(\"\\n--- Script Execution Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:21:18.427848Z","iopub.execute_input":"2025-04-13T22:21:18.428223Z","iopub.status.idle":"2025-04-13T22:22:11.313193Z","shell.execute_reply.started":"2025-04-13T22:21:18.428197Z","shell.execute_reply":"2025-04-13T22:22:11.31184Z"}},"outputs":[{"name":"stdout","text":"Imports successful.\nOpenAI API Key retrieved successfully.\nLLM (gpt-4-turbo) initialized.\n\n--- Processing Manuscript ---\nLoading PDF: /kaggle/input/sample-manuscript/Sample-Manuscript.pdf\nSuccessfully loaded and cleaned text from: /kaggle/input/sample-manuscript/Sample-Manuscript.pdf (1 pages, 881 chars)\nPrepared fallback title from filename: Sample Manuscript\nError during LLM title extraction: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Using fallback: Sample Manuscript\nGenerating summary...\nError during summarization: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nExtracting specific data points from manuscript text...\nRequesting LLM extraction from Manuscript PDF...\nError during LLM data extraction from Manuscript PDF: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Manuscript Summary ---\nError during summarization: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Manuscript Extracted Data ---\n       corrosion_inhibition_%           impedance_ohm_cm2  \\\n0  LLM Error (Manuscript PDF)  LLM Error (Manuscript PDF)   \n\n                 adhesion_MPa  \n0  LLM Error (Manuscript PDF)  \n\n--- Searching for Similar Articles (Title: 'Sample Manuscript') ---\nSearching Google for: 'Sample Manuscript' (Top 20)\nFound 20 potential URLs.\n\n--- Processing URL (1/20): https://journals.lww.com/greenjournal/Documents/SampleManuscript.pdf ---\nAttempting direct PDF download from: https://journals.lww.com/greenjournal/Documents/SampleManuscript.pdf\nSuccessfully downloaded and verified PDF (551469 bytes) to: /kaggle/working/temp_pdfs/tmp0mfnhcnr.pdf\nLoading PDF: /kaggle/working/temp_pdfs/tmp0mfnhcnr.pdf\nSuccessfully loaded and cleaned text from: /kaggle/working/temp_pdfs/tmp0mfnhcnr.pdf (6 pages, 4710 chars)\nRequesting LLM extraction from PDF...\nError during LLM data extraction from PDF: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nRemoved temporary file: /kaggle/working/temp_pdfs/tmp0mfnhcnr.pdf\n\n--- Processing URL (2/20): https://ijcsrr.org/manuscript-template/ ---\nAttempting direct PDF download from: https://ijcsrr.org/manuscript-template/\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html; charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://ijcsrr.org/manuscript-template/ for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~9205 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Processing URL (3/20): https://journal.naturalhistoryinstitute.org/wp-content/uploads/2011/07/An-Example-Manuscript.v2.pdf ---\nAttempting direct PDF download from: https://journal.naturalhistoryinstitute.org/wp-content/uploads/2011/07/An-Example-Manuscript.v2.pdf\nFailed download/access for https://journal.naturalhistoryinstitute.org/wp-content/uploads/2011/07/An-Example-Manuscript.v2.pdf: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n\n--- Processing URL (4/20): https://sib.illinois.edu/undergraduate/guidelines/manuscript ---\nAttempting direct PDF download from: https://sib.illinois.edu/undergraduate/guidelines/manuscript\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html; charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://sib.illinois.edu/undergraduate/guidelines/manuscript for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~10985 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Processing URL (5/20): https://self-publishingschool.com/manuscript-format/ ---\nAttempting direct PDF download from: https://self-publishingschool.com/manuscript-format/\nFailed download/access for https://self-publishingschool.com/manuscript-format/: 403 Client Error: Forbidden for url: https://self-publishingschool.com/manuscript-format/\n\n--- Processing URL (6/20): https://www.medicinearticle.com/JMR_Manuscript_template.doc ---\nAttempting direct PDF download from: https://www.medicinearticle.com/JMR_Manuscript_template.doc\nURL does not appear to be a direct PDF. Status: 200, Content-Type: application/msword\n\n--- Processing URL (7/20): https://www.scribophile.com/academy/how-to-format-a-novel-manuscript ---\nAttempting direct PDF download from: https://www.scribophile.com/academy/how-to-format-a-novel-manuscript\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html; charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://www.scribophile.com/academy/how-to-format-a-novel-manuscript for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~8619 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Processing URL (8/20): https://apastyle.apa.org/style-grammar-guidelines/paper-format/sample-papers ---\nAttempting direct PDF download from: https://apastyle.apa.org/style-grammar-guidelines/paper-format/sample-papers\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html\nDirect download failed, content is HTML.\nScanning HTML from https://apastyle.apa.org/style-grammar-guidelines/paper-format/sample-papers for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~0 characters of text from HTML.\n\n--- Processing URL (9/20): https://spie.org/documents/Publications/ProcSPIETemplate_A4.pdf ---\nAttempting direct PDF download from: https://spie.org/documents/Publications/ProcSPIETemplate_A4.pdf\nSuccessfully downloaded and verified PDF (431294 bytes) to: /kaggle/working/temp_pdfs/tmpfnx08_1z.pdf\nLoading PDF: /kaggle/working/temp_pdfs/tmpfnx08_1z.pdf\nSuccessfully loaded and cleaned text from: /kaggle/working/temp_pdfs/tmpfnx08_1z.pdf (5 pages, 8161 chars)\nRequesting LLM extraction from PDF...\nError during LLM data extraction from PDF: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nRemoved temporary file: /kaggle/working/temp_pdfs/tmpfnx08_1z.pdf\n\n--- Processing URL (10/20): https://legacyfileshare.elsevier.com/promis_misc/ijsr-manuscript-format.pdf ---\nAttempting direct PDF download from: https://legacyfileshare.elsevier.com/promis_misc/ijsr-manuscript-format.pdf\nSuccessfully downloaded and verified PDF (551860 bytes) to: /kaggle/working/temp_pdfs/tmprdldk6pq.pdf\nLoading PDF: /kaggle/working/temp_pdfs/tmprdldk6pq.pdf\nSuccessfully loaded and cleaned text from: /kaggle/working/temp_pdfs/tmprdldk6pq.pdf (7 pages, 10473 chars)\nRequesting LLM extraction from PDF...\nError during LLM data extraction from PDF: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nRemoved temporary file: /kaggle/working/temp_pdfs/tmprdldk6pq.pdf\n\n--- Processing URL (11/20): http://web.mit.edu/kjb/mitconf/Sample_Manuscript.PDF ---\nAttempting direct PDF download from: http://web.mit.edu/kjb/mitconf/Sample_Manuscript.PDF\nSuccessfully downloaded and verified PDF (12387 bytes) to: /kaggle/working/temp_pdfs/tmpe29ltxp0.pdf\nLoading PDF: /kaggle/working/temp_pdfs/tmpe29ltxp0.pdf\nSuccessfully loaded and cleaned text from: /kaggle/working/temp_pdfs/tmpe29ltxp0.pdf (4 pages, 3722 chars)\nRequesting LLM extraction from PDF...\nError during LLM data extraction from PDF: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nRemoved temporary file: /kaggle/working/temp_pdfs/tmpe29ltxp0.pdf\n\n--- Processing URL (12/20): https://www.aliem.com/template-journal-manuscript/ ---\nAttempting direct PDF download from: https://www.aliem.com/template-journal-manuscript/\nFailed download/access for https://www.aliem.com/template-journal-manuscript/: 403 Client Error: Forbidden for url: https://www.aliem.com/template-journal-manuscript/\n\n--- Processing URL (13/20): https://www.shunn.net/format/novel/ ---\nAttempting direct PDF download from: https://www.shunn.net/format/novel/\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html\nDirect download failed, content is HTML.\nScanning HTML from https://www.shunn.net/format/novel/ for PDF links.\nFound potential PDF link: https://www.shunn.net/format/pdf/ManuscriptFormat_Novel_WilliamShunn.pdf\nFound 1 potential links. Attempting download...\nAttempting download from HTML link: https://www.shunn.net/format/pdf/ManuscriptFormat_Novel_WilliamShunn.pdf\nAttempting direct PDF download from: https://www.shunn.net/format/pdf/ManuscriptFormat_Novel_WilliamShunn.pdf\nSuccessfully downloaded and verified PDF (66966 bytes) to: /kaggle/working/temp_pdfs/tmpqj_d2gex.pdf\nSuccessfully downloaded PDF from HTML link: https://www.shunn.net/format/pdf/ManuscriptFormat_Novel_WilliamShunn.pdf\nLoading PDF: /kaggle/working/temp_pdfs/tmpqj_d2gex.pdf\nSuccessfully loaded and cleaned text from: /kaggle/working/temp_pdfs/tmpqj_d2gex.pdf (19 pages, 26140 chars)\nRequesting LLM extraction from PDF (from HTML link)...\nError during LLM data extraction from PDF (from HTML link): Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nRemoved temporary file: /kaggle/working/temp_pdfs/tmpqj_d2gex.pdf\n\n--- Processing URL (14/20): https://www.ieee.org/conferences/publishing/templates.html ---\nAttempting direct PDF download from: https://www.ieee.org/conferences/publishing/templates.html\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html;charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://www.ieee.org/conferences/publishing/templates.html for PDF links.\nFound potential PDF link: http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/IEEEtran_HOWTO.pdf\nFound 1 potential links. Attempting download...\nAttempting download from HTML link: http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/IEEEtran_HOWTO.pdf\nAttempting direct PDF download from: http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/IEEEtran_HOWTO.pdf\nSuccessfully downloaded and verified PDF (671626 bytes) to: /kaggle/working/temp_pdfs/tmpcv07onm8.pdf\nSuccessfully downloaded PDF from HTML link: http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/IEEEtran_HOWTO.pdf\nLoading PDF: /kaggle/working/temp_pdfs/tmpcv07onm8.pdf\nSuccessfully loaded and cleaned text from: /kaggle/working/temp_pdfs/tmpcv07onm8.pdf (28 pages, 149458 chars)\nRequesting LLM extraction from PDF (from HTML link)...\nError during LLM data extraction from PDF (from HTML link): Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\nRemoved temporary file: /kaggle/working/temp_pdfs/tmpcv07onm8.pdf\n\n--- Processing URL (15/20): https://blog.reedsy.com/book-manuscript-format/ ---\nAttempting direct PDF download from: https://blog.reedsy.com/book-manuscript-format/\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html; charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://blog.reedsy.com/book-manuscript-format/ for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~17549 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Processing URL (16/20): https://authorservices.taylorandfrancis.com/publishing-your-research/writing-your-paper/journal-manuscript-layout-guide/ ---\nAttempting direct PDF download from: https://authorservices.taylorandfrancis.com/publishing-your-research/writing-your-paper/journal-manuscript-layout-guide/\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html; charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://authorservices.taylorandfrancis.com/publishing-your-research/writing-your-paper/journal-manuscript-layout-guide/ for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~11223 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Processing URL (17/20): https://www.dovepress.com/author-guidelines/manuscript-template ---\nAttempting direct PDF download from: https://www.dovepress.com/author-guidelines/manuscript-template\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html; charset=utf-8\nDirect download failed, content is HTML.\nScanning HTML from https://www.dovepress.com/author-guidelines/manuscript-template for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~5289 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n\n--- Processing URL (18/20): https://websites.uwlax.edu/biology/ResearchManuscripts.html ---\nAttempting direct PDF download from: https://websites.uwlax.edu/biology/ResearchManuscripts.html\nURL does not appear to be a direct PDF. Status: 200, Content-Type: text/html\nDirect download failed, content is HTML.\nScanning HTML from https://websites.uwlax.edu/biology/ResearchManuscripts.html for PDF links.\nNo likely PDF links found in HTML.\nCould not download PDF from HTML links. Attempting extraction from HTML text.\nExtracted ~9834 characters of text from HTML.\nRequesting LLM extraction from HTML...\nError during LLM data extraction from HTML: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2896720102.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# Politeness delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Slightly increased delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16}]}